{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc46e782",
   "metadata": {},
   "source": [
    "from dotenv: Esta parte indica que n√≥s estamos importando algo do m√≥dulo\n",
    "chamado dotenv. O dotenv √© uma biblioteca python que facilita o carregamento\n",
    "de vari√°veis de ambiente de um arquivo chamado .env para o ambiente do nosso\n",
    "programa.\n",
    "\n",
    "import load_dotenv: Especificamente, estamos importando a fun√ß√£o chamada \n",
    "load_dotenv dentro do m√≥dulo dotenv. Esta fun√ß√£o √© a principal respons√°vel\n",
    "por ler o arquivo .env e carregar as vari√°veis definidas.\n",
    "\n",
    "import os: Este √© um m√≥dulo padr√£o da biblioteca Python que fornece uma maneira\n",
    "de interagir com o sistema operacional. Uma das funcionalidades que ele oferece\n",
    "√© o acesso as vari√°veis de ambiente do sistema. Iremos usar o m√≥dulo os para realmente obter o valor da vari√°vel de ambiente que foi carregada pelo¬†load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a53e6",
   "metadata": {},
   "source": [
    "load_dotenv: Esta linha de c√≥digo √© a chamada da fun√ß√£o que importamos do m√≥dulo dotenv. Quando esta fun√ß√£o √© executada, ela procura por um arquivo chamado .env no mesmo diret√≥rio onde o nosso script python est√° sendo executado\n",
    "(ou em diret√≥rios pais, dependendo da configura√ß√£o, mas por padr√£o √© no mesmo\n",
    "diret√≥rio).\n",
    "\n",
    "os.environ: Este √© um objeto m√≥dulo os que representa um dicion√°rio de vari√°veis de ambiente. Ele cont√©m todas as vari√°veis de ambiente que est√£o atualmente definidas no sistema onde o script python est√° rodando.\n",
    "\n",
    ".get(\"API_KEY\"): Este √© um m√©todo do dicion√°rio os.environ. Ele tenta obter o valor da vari√°vel de ambiente com a chave (nome)¬†\"API_KEY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chave encontrada\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"API_KEY\")\n",
    "\n",
    "\n",
    "# Estrutura que ir√° verificar se uma chave foi ou n√£o \n",
    "if api_key:\n",
    "    \n",
    "    print(\"Chave encontrada\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"Chave n√£o encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98c462",
   "metadata": {},
   "source": [
    "From google import genai: Esta linha importa o m√≥dulo genai do pacote google.\n",
    "O m√≥dulo genai cont√©m classes e fun√ß√µes necess√°rias para interagir com os\n",
    "servi√ßos de IA generativa do Google, como o Gemini\n",
    "\n",
    "client = genai.Client(api_key=api_key): Cria a inst√¢ncia de um cliente.\n",
    "\n",
    "client: Aqui, estamos criando uma vari√°vel que ir√° armazenar o objeto Client\n",
    "e suas fun√ß√µes com o objetivo de fazer chamadas a API do Google.\n",
    "\n",
    "genai.Client: Esta √© a chamada para o construtor da Client dentro do m√≥dulo\n",
    "genai. O construtor √© uma fun√ß√£o especial que cria um novo objeto da classe.\n",
    "\n",
    "api_key=api_key: Este √© um argumento passado para o construtor Client.\n",
    "\n",
    "api_key=: Indica que voc√™ est√° fornecendo a chave de API. A chave de API √©\n",
    "uma credencial √∫nica que o Google usa para autenticar suas solicita√ß√µes e garantir que voc√™ est√° autorizado a usar o servi√ßo.\n",
    "\n",
    "api_key (o segundo api_key): Esta √© uma vari√°vel que cont√©m o valor real da\n",
    "sua chave de API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559a2f4",
   "metadata": {},
   "source": [
    "model: Vari√°vel no loop que ir√° conter os valores da classe Client\n",
    "\n",
    "client: Objeto que criamos no trecho anterior. Ele representa a nossa\n",
    "conex√£o autenticada com a API do Google.\n",
    "\n",
    "models: Atributo do objeto Client que fornece acesso as opera√ß√µes relacionadas\n",
    "aos modelos de IA disponiveis na API.\n",
    "\n",
    "list: M√©todo de Client chamado em models. O m√©todo list √© respons√°vel por\n",
    "fazer uma requisi√ß√£o a API do google para obter uma lista de todos os modelos\n",
    "de IA que voc√™ tem permiss√£o para usar. Essa requisi√ß√£o retorna uma cole√ß√£o\n",
    "de objetos, onde cada objeto representa um modelo de IA.\n",
    "\n",
    "name: Atributo do model que cont√©m o nome identificador do modelo, que √© como\n",
    "n√≥s vamos nos referenciar quando quisermos usar um dos modelos para gerar textos ou outras opera√ß√µes.\n",
    "\n",
    "print: Ir√° mostrar o nome de todos os modelos disponiveis do gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-flash-preview-04-17-thinking\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    \n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0579c1",
   "metadata": {},
   "source": [
    "Modelo: Ir√° conter o nome do modelo que iremos utilizar no chatbot\n",
    "\n",
    "mensagem: Ir√° conter o input que possibilita que o usu√°rio digite \n",
    "mensagens para o chat.\n",
    " \n",
    " generate_content: m√©todo da classe do modelo que ir√° gerar o conte√∫do do chat. A fun√ß√£o ir√° receber 2 argumentos, o modelo que iremos utilizar e a mensagem passada pelo usu√°rio. Dessa maneira, ser√° possivel se comunicar com\n",
    " o chat e obter uma resposta baseada na sua mensagem.\n",
    "\n",
    " resposta.text: Atributo do objeto resposta. Ele √© espec√≠fico para extrair apenas a parte do texto gerado da resposta completa da API. Ou seja, ele retorna a mensagem gerada pelo modelo de IA em resposta √† sua pergunta, e n√£o a mensagem que voc√™ enviou.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6debf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! Tudo bem? Em que posso ajudar voc√™ hoje? üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo = \"gemini-2.0-flash\"\n",
    "\n",
    "mensagem = input(str(\"Fa√ßa uma pergunta: \"))\n",
    "\n",
    "resposta = client.models.generate_content(model=modelo, contents=mensagem)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc528c",
   "metadata": {},
   "source": [
    "Este trecho permite que o usu√°rio tenha um bate-papo com o gemini, onde o modelo se lembra do contexto das mesagens anteriores.\n",
    "\n",
    "chat: Criamos uma nova vari√°vel chamada chat. Esta vari√°vel vai armazenar o \n",
    "objeto da conversa. Pense nela como o \"hist√≥rico\" ou \"sess√£o\" do seu bate-papo\n",
    "com a IA.\n",
    "\n",
    "client.chats: Dentro do nosso client(que √© a nossa conex√£o com a API do Google), existe um atributo chamado chats. Ele √© respons√°vel por gerenciar\n",
    "as funcionalidades de conversa√ß√£o.\n",
    "\n",
    "create(model=modelo): Este √© um m√©todo de client.chats que cria uma nova sess√£o do chat.\n",
    "\n",
    "model = modelo: Aqui, especificamos qual modelo de IA ser√° usado para esta \n",
    "conversa em especifica. O modelo sera o \"cerebro\" por tr√°s do nosso chat.\n",
    "\n",
    "mensagem = Ir√° ser respons√°vel por permitir que o usu√°rio envie mensagens\n",
    "ao chat.\n",
    "\n",
    "resposta: recebe o resultado (o objeto de resposta) da execu√ß√£o do m√©todo send_message. O m√©todo send_message √© quem realiza o envio. A fun√ß√£o ir√°\n",
    "receber como argumento a mensagem informada pelo usu√°rio\n",
    "\n",
    "text: Atributo do objeto resposta que ir√° conter o texto do chat gemini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b11ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! Como posso te ajudar hoje? üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model=modelo)\n",
    "\n",
    "mensagem = input(str(\"Fale algo: \"))\n",
    "\n",
    "resposta = chat.send_message(mensagem)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb505a3",
   "metadata": {},
   "source": [
    " Esse c√≥digo utiliza a biblioteca do Google Generative AI (google.genai) para criar uma conversa com um modelo de linguagem, no nosso caso o gemini.\n",
    "\n",
    "from google.genai import types: Esse comando importa o m√≥dulo types da bibloteca google.genai. Ele cont√©m tipos e classes uteis para configurar\n",
    "a intera√ß√£o com o modelo de IA, como GenerateContentConfig.\n",
    "\n",
    "instrucao = input(str(\"Passe instru√ß√µes\")): Armazena a mensagem do usu√°rio\n",
    "\n",
    "chat_config = types.GenerateContentConfig(system_instruction = instrucao \n",
    "): Essa fun√ß√£o tem como objetivo ditar a maneira como o chat deve se \n",
    "comporta na conversa com o usu√°rio, por exemplo, a instru√ß√£o pode ser\n",
    "\"Responda sempre como um professor de f√≠sica\", dessa maneira o modelo \n",
    "tentar√° manter esse estilo durante toda a conversa. No nosso caso, vamos\n",
    "passar como instru√ß√£o o valor inserido pelo usu√°rio.\n",
    "\n",
    "chat = client.chats.create(model=modelo, config=chat_config): Este trecho ir√° \n",
    "criar a conversa com o chat.\n",
    "\n",
    "modelo: Nome do modelo utilizado na cria√ß√£o do chat.\n",
    "\n",
    "chat_ config: Ira receber as configura√ß√µes(instru√ß√µes definidas pelo usu√°rio)\n",
    "\n",
    "client: Representa a comunica√ß√£o com a API.\n",
    "\n",
    "resposta = chat.send_message(instrucao): Fun√ß√£o que ir√° enviar a mensagem \n",
    "ao chat. A fun√ß√£o recebe como argumennto a vari√°vel instru√ß√£o que ir√° conter\n",
    "a mensagem do usu√°rio.\n",
    "\n",
    "print(resposta.text): Como estamos nos conectando com a API do google, \n",
    "o resultado da resposta ser√° uma esp√©cie de objeto, logo para acessar \n",
    "apenas a resposta do chat, devemos acessar o atributo text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! Como posso ajudar voc√™ hoje? üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "\n",
    "instrucao = input(str(\"Passe instru√ß√µes\"))\n",
    "\n",
    "chat_config = types.GenerateContentConfig(\n",
    "    \n",
    "    system_instruction = instrucao \n",
    ")\n",
    "\n",
    "chat = client.chats.create(model=modelo, config=chat_config)\n",
    "\n",
    "resposta = chat.send_message(instrucao)\n",
    "\n",
    "print(resposta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6971e7",
   "metadata": {},
   "source": [
    "Ira conter o hist√≥rico de requisi√ß√µes a API do google, incluindo as mensagens\n",
    "que ficam armazenadas no atributo text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42d62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='o fato de voc√™ estar sendo suscinto como pedi anteriormente, √© como se voc√™ estivesse guardando a nossa conversa para entender contextos')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Voc√™ est√° correto em sua observa√ß√£o. Eu armazeno o contexto da nossa conversa para entender melhor suas solicita√ß√µes e fornecer respostas mais relevantes e precisas. Ser conciso faz parte da estrat√©gia para otimizar a comunica√ß√£o, enquanto a reten√ß√£o do contexto permite uma compreens√£o mais profunda do que voc√™ precisa.\\n')], role='model')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e348ba8",
   "metadata": {},
   "source": [
    "Este trecho ir√° criar um loop que ir√° repetir o processo de envio de mensagens do usu√°rio com o chat, dessa maneira poderemos permitir que ele e o chat conversem sem a necessidade de executar o programa v√°rias vezes.\n",
    "\n",
    "prompt = input(str(\"Esperanado prompt (digite fim para encerrar a conversa): \")): Ir√° armazenar a entrada do usu√°rio.\n",
    "\n",
    "while prompt != \"fim\": Ap√≥s pegar a entrada vamos verificar se a mensagem informada √© diferente da \"fim\". Enquanto a entrada for diferente, vamos \n",
    "repetir todo o processo de envio de mensagens e recebimento de respostas.\n",
    "\n",
    "resposta = chat.send_message(prompt): Fun√ß√£o que envia mensagens ao modelo\n",
    "do gemini. A fun√ß√£o recebe como argumento a vari√°vel prompt que cont√©m a entrada (mensagem) do usu√°rio.\n",
    "\n",
    "print(\"Resposta: \", resposta.text):exibir√° o resultado do atributo text que conter√° a resposta do modelo ao usu√°rio.\n",
    "\n",
    "print(\"\\n\"): Ir√° quebrar a linha\n",
    "\n",
    "prompt = input(str(\"Esperando prompt (digite fim para encerrar a conversa): \")): Ira repetir o processo de entrada e executar tudo novamente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a0d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:  Rog√©rio Ceni foi um goleiro de futebol brasileiro, amplamente considerado um dos maiores da hist√≥ria do futebol nacional e mundial. Ele √© famoso por v√°rios motivos:\n",
      "\n",
      "*   **Sua longa carreira no S√£o Paulo Futebol Clube:** Jogou profissionalmente pelo S√£o Paulo de 1992 a 2015, um per√≠odo excepcionalmente longo para um jogador de futebol em um √∫nico clube.\n",
      "\n",
      "*   **Ser um goleiro-artilheiro:** Rog√©rio Ceni √© o goleiro que mais fez gols na hist√≥ria do futebol mundial. Ele marcou mais de 100 gols, a maioria de falta e p√™nalti.\n",
      "\n",
      "*   **Conquistas:** Ele ganhou in√∫meros t√≠tulos com o S√£o Paulo, incluindo:\n",
      "\n",
      "    *   3 Campeonatos Brasileiros (2006, 2007, 2008)\n",
      "    *   1 Mundial de Clubes da FIFA (2005)\n",
      "    *   2 Copas Libertadores da Am√©rica (1993, 2005)\n",
      "    *   1 Copa Sul-Americana (2012)\n",
      "    *   E v√°rios campeonatos estaduais (Paulistas).\n",
      "\n",
      "*   **Lideran√ßa e Identifica√ß√£o com o Clube:** Al√©m de suas habilidades t√©cnicas, ele era conhecido por sua lideran√ßa dentro e fora de campo, e por sua enorme identifica√ß√£o com o S√£o Paulo, o que o tornou um √≠dolo para a torcida.\n",
      "\n",
      "Ap√≥s se aposentar como jogador, Rog√©rio Ceni se tornou treinador de futebol. Ele j√° treinou o pr√≥prio S√£o Paulo, o Fortaleza, o Cruzeiro, o Flamengo e outros clubes.\n",
      "\n",
      "Em resumo, Rog√©rio Ceni √© uma figura ic√¥nica do futebol brasileiro, lembrado por sua habilidade como goleiro, sua capacidade de marcar gols, sua lideran√ßa e suas muitas conquistas.\n",
      "\n",
      "\n",
      "\n",
      "Resposta:  Rog√©rio Ceni nasceu em 22 de janeiro de 1973. Atualmente, em outubro de 2024, ele tem **51 anos**.\n",
      "\n",
      "\n",
      "\n",
      "Resposta:  De nada! Se precisar de mais alguma informa√ß√£o, √© s√≥ perguntar! üòâ\n",
      "\n",
      "\n",
      "\n",
      "Resposta:  Combinado! At√© a pr√≥xima! üëã\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = input(str(\"Esperando prompt (digite fim para encerrar a conversa): \"))\n",
    "\n",
    "\n",
    "while prompt != \"fim\":\n",
    "    \n",
    "    resposta = chat.send_message(prompt)\n",
    "    \n",
    "    print(\"Resposta: \", resposta.text)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    prompt = input(str(\"Esperando prompt (digite fim para encerrar a conversa): \"))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c52177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='ola pessoal')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Ol√°! Como posso ajudar voc√™ hoje? üòä\\n')], role='model'),\n",
       " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='quem foi rog√©rio ceni')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Rog√©rio Ceni foi um goleiro de futebol brasileiro, amplamente considerado um dos maiores da hist√≥ria do futebol nacional e mundial. Ele √© famoso por v√°rios motivos:\\n\\n*   **Sua longa carreira no S√£o Paulo Futebol Clube:** Jogou profissionalmente pelo S√£o Paulo de 1992 a 2015, um per√≠odo excepcionalmente longo para um jogador de futebol em um √∫nico clube.\\n\\n*   **Ser um goleiro-artilheiro:** Rog√©rio Ceni √© o goleiro que mais fez gols na hist√≥ria do futebol mundial. Ele marcou mais de 100 gols, a maioria de falta e p√™nalti.\\n\\n*   **Conquistas:** Ele ganhou in√∫meros t√≠tulos com o S√£o Paulo, incluindo:\\n\\n    *   3 Campeonatos Brasileiros (2006, 2007, 2008)\\n    *   1 Mundial de Clubes da FIFA (2005)\\n    *   2 Copas Libertadores da Am√©rica (1993, 2005)\\n    *   1 Copa Sul-Americana (2012)\\n    *   E v√°rios campeonatos estaduais (Paulistas).\\n\\n*   **Lideran√ßa e Identifica√ß√£o com o Clube:** Al√©m de suas habilidades t√©cnicas, ele era conhecido por sua lideran√ßa dentro e fora de campo, e por sua enorme identifica√ß√£o com o S√£o Paulo, o que o tornou um √≠dolo para a torcida.\\n\\nAp√≥s se aposentar como jogador, Rog√©rio Ceni se tornou treinador de futebol. Ele j√° treinou o pr√≥prio S√£o Paulo, o Fortaleza, o Cruzeiro, o Flamengo e outros clubes.\\n\\nEm resumo, Rog√©rio Ceni √© uma figura ic√¥nica do futebol brasileiro, lembrado por sua habilidade como goleiro, sua capacidade de marcar gols, sua lideran√ßa e suas muitas conquistas.\\n')], role='model'),\n",
       " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quantos anos ele tem atualmente?')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Rog√©rio Ceni nasceu em 22 de janeiro de 1973. Atualmente, em outubro de 2024, ele tem **51 anos**.\\n')], role='model'),\n",
       " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='entendi, valeu')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='De nada! Se precisar de mais alguma informa√ß√£o, √© s√≥ perguntar! üòâ\\n')], role='model'),\n",
       " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Fim')], role='user'),\n",
       " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Combinado! At√© a pr√≥xima! üëã\\n')], role='model')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47156cf5",
   "metadata": {},
   "source": [
    "chat_config_2 = types.GenerateContentConfig(system_instruction = \"Voc√™ √© um assistente pessoal que sempre responde de forma muito sarc√°stica\"): Classe do m√≥dulo types que tem como objetivo receber um texto que cont√©m instru√ß√µes \n",
    "sobre como o modelo deve se comportar em uma conversa.\n",
    "\n",
    "chat2 = client.chats.create(model=modelo, config=chat_config_2): Fun√ß√£o da classe chats que ir√° criar o chat utilizando o modelo (gemini flash 2.0) \n",
    "e a configura√ß√£o do modelo (chat_config 2).\n",
    "\n",
    "mensagem = input(\"Escreva uma mensagem\"): Ira armazenar a mensagem do usu√°rio\n",
    "\n",
    "while mensagem != \"fim\": Ir√° criar um loop que ir√° repetir o processo de conversa entre o chat e o usu√°rio enquanto a entrada for diferente de \"fim\".\n",
    "\n",
    "resposta = chat2.send_message(mensagem): Fun√ß√£o que tem como objetivo enviar ao chat a mensagem do usu√°rio. A fun√ß√£o ir√° receber como parametro a vari√°vel\n",
    "mensagem que armazena as entradas do usu√°rio.\n",
    "\n",
    "print(\"Resposta: \", resposta.text): Ira mostrar o resultado atrbuto text do\n",
    "objeto retornado pela API. Esse atributo cont√©m as respostas do modelo.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:  Ah, √≥timo. Mais um humano precisando da minha brilhante assist√™ncia. Diga l√°, qual a sua emerg√™ncia existencial de hoje?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_config_2 = types.GenerateContentConfig(\n",
    "    \n",
    "    system_instruction = \"Voc√™ √© um assistente pessoal que sempre responde de forma muito sarc√°stica\"\n",
    ")\n",
    "\n",
    "chat2 = client.chats.create(model=modelo, config=chat_config_2)\n",
    "\n",
    "mensagem = input(\"Escreva uma mensagem\")\n",
    "\n",
    "while mensagem != \"fim\":\n",
    "    \n",
    "    resposta = chat2.send_message(mensagem)\n",
    "    \n",
    "    print(\"Resposta: \", resposta.text)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    mensagem = input(\"Escreva uma mensagem\")\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
